# Bias Detection in LLM Data Narratives  
**Syracuse University â€“ Research Task 8**

---

##  Executive Summary
This study investigated whether Large Language Models (LLMs) demonstrate framing, demographic, or confirmation bias when interpreting identical sports statistics.  
Using anonymized SU Womenâ€™s Lacrosse performance data (2020 â€“ 2025), we tested whether minor wording changes in promptsâ€”such as *â€œstrugglingâ€* vs *â€œdevelopingâ€*â€”led to differing narratives and recommendations.  
Results showed consistent sentiment shifts across prompt framings, confirming that linguistic framing can meaningfully alter model conclusions even when the underlying data remain constant.

---

##  Methodology
- **Dataset:** Aggregated player performance metrics (Goals, Assists, Turnovers, etc.) from anonymized lacrosse data  
- **Model:** Groq `llama-3.1-8b-instant` (October 2025 build)  
- **Prompt Groups:**  
  - **H1:** Positive vs Negative Framing  
  - **H2:** Demographic Mention vs Neutral  
  - **H3:** Confirmation Bias Priming (â€œwhat went wrongâ€ vs â€œwhat can improveâ€)  
  - **H4â€“H5:** Control and Open Exploration prompts  
- **Metrics:** Sentiment polarity, keyword bias counts, factual validation rate  
- **Tools:** Python 3.11 Â· Pandas Â· TextBlob Â· Matplotlib Â· Groq API  

---

##  Results
Supporting files:  
- `analysis/bias_summary.csv`  
- `analysis/sentiment_plot.png`  
- `analysis/validation_report.csv`

### Key Findings
- **Positive framings** produced higher sentiment and more â€œpotential/improvementâ€ language.  
- **Negative framings** increased use of â€œweak/poor/struggleâ€ descriptors.  
- **Confirmation-primed prompts** selectively referenced favorable statistics.  
- **Validation accuracy:** factual alignment ranged from 0â€“10 % (frames/demographics) to â‰ˆ50 % (problem-focused prompts).

---

##  Bias Catalogue
| Bias Type | Observed Effect | Severity |
|------------|----------------|-----------|
| **Framing Bias** | Emotional tone changes with wording | ğŸ”´ High |
| **Confirmation Bias** | Model reinforces primed hypothesis | ğŸŸ  Medium |
| **Selection Bias** | Uneven emphasis on certain statistics | ğŸŸ¡ Low |

---

##  Mitigation Strategies
1. Use **neutral** or **data-anchored** phrasing (e.g., â€œSummarize trends in player metricsâ€).  
2. Present **quantitative tables** inside prompts to constrain interpretation.  
3. **Cross-validate** narratives across multiple LLM providers.  
4. Apply **sentiment normalization** or **re-prompting** when tone skew exceeds threshold.  

---

##  Limitations
- Evaluated a **single model** (Groq LLaMA 3.1 8B).  
- Focused on **single-turn** responses; multi-turn conversational drift not assessed.  
- Sentiment analysis limited to **TextBlob polarity** (rule-based).  
- Qualitative interpretation still requires **human review**.

---

##  Future Work
- Extend to **Claude 3 and GPT-4o** for cross-model bias comparison.  
- Incorporate **SHAP-based explainability** on text embeddings.  
- Build a **Streamlit dashboard** for interactive bias visualization.

---

**Author:** Lakshmi Peram  
**Course:** IST Research Task 8 â€“ Bias Detection in LLM Data Narratives  
**Submission Deadline:** November 15  2025  
